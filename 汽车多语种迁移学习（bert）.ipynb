{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52282dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_cn = pd.read_excel('./中文_trian.xlsx')\n",
    "train_ja = pd.read_excel('./日语_train.xlsx')\n",
    "train_en = pd.read_excel('./英文_train.xlsx')\n",
    "\n",
    "test_ja = pd.read_excel('./testA.xlsx', sheet_name='日语_testA')\n",
    "test_en = pd.read_excel('./testA.xlsx', sheet_name='英文_testA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9911f0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>原始文本</th>\n",
       "      <th>意图</th>\n",
       "      <th>槽值1</th>\n",
       "      <th>槽值2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.5度</td>\n",
       "      <td>adjust_ac_temperature_to_number</td>\n",
       "      <td>offset:16.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16度</td>\n",
       "      <td>adjust_ac_temperature_to_number</td>\n",
       "      <td>offset:16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    原始文本                               意图          槽值1  槽值2\n",
       "0  16.5度  adjust_ac_temperature_to_number  offset:16.5  NaN\n",
       "1    16度  adjust_ac_temperature_to_number    offset:16  NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn[~train_cn['槽值1'].isnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab7b0145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>原始文本</th>\n",
       "      <th>中文翻译</th>\n",
       "      <th>意图</th>\n",
       "      <th>槽值1</th>\n",
       "      <th>槽值2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>エアコンの冷房モードをオンにして</td>\n",
       "      <td>开启空调的制冷模式</td>\n",
       "      <td>open_ac_mode</td>\n",
       "      <td>mode:冷房モード</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>エアコンの冷房モードをつけて</td>\n",
       "      <td>开启空调的制冷模式</td>\n",
       "      <td>open_ac_mode</td>\n",
       "      <td>mode:冷房モード</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                原始文本       中文翻译            意图         槽值1  槽值2\n",
       "35  エアコンの冷房モードをオンにして  开启空调的制冷模式  open_ac_mode  mode:冷房モード  NaN\n",
       "36    エアコンの冷房モードをつけて  开启空调的制冷模式  open_ac_mode  mode:冷房モード  NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ja[~train_ja['槽值1'].isnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c86fca38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 33, 71)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn['原始文本'].apply(len).max(), train_ja['原始文本'].apply(len).max(), train_en['原始文本'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58bdb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# AutoTokenizer：分词器\n",
    "\n",
    "# Auto：自动识别的\n",
    "model_name = \"bert-base-chinese\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdccee8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f318435e3745422cbcf686608e6a2c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5cea2df1e04409bcc2d1a480521c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, BertModel, AutoModel\n",
    "model = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8d1f916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2090,  0.2921,  0.2075,  ..., -0.3150,  0.0329, -0.0352],\n",
       "         [ 0.1141,  0.5552,  0.6361,  ...,  0.2087,  0.0772, -0.8664],\n",
       "         [ 0.8204,  0.2228,  0.3018,  ...,  0.1404, -0.2797, -0.0781],\n",
       "         ...,\n",
       "         [ 0.4625,  0.1090, -0.1031,  ..., -0.0032, -0.2640, -0.5389],\n",
       "         [ 0.4616,  0.1152, -0.1214,  ...,  0.0076, -0.2020, -0.6232],\n",
       "         [ 0.4785,  0.1196, -0.0029,  ...,  0.0727, -0.2967, -0.5149]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.9966,  0.9605,  0.9827,  0.9573,  0.7397,  0.2169, -0.9281, -0.2038,\n",
       "          0.5496, -0.9845,  0.9994,  0.9872, -0.6430, -0.8330,  0.9501, -0.9907,\n",
       "          0.6900,  0.6050,  0.2719, -0.6332,  0.9893, -0.9912, -0.8960,  0.2563,\n",
       "         -0.0675,  0.7463,  0.7332,  0.4835, -0.9986,  0.9761,  0.7391,  0.9935,\n",
       "          0.4609, -0.9963, -0.9994,  0.5128,  0.3549,  0.9656,  0.5940, -0.9801,\n",
       "         -0.8955,  0.1684, -0.4020, -0.9433,  0.6816,  0.6676, -0.9977, -0.9954,\n",
       "         -0.4095,  0.9643, -0.7874, -0.9935,  0.8698, -0.4265, -0.2736,  0.9557,\n",
       "         -0.9896,  0.0345,  0.9962,  0.0158,  0.9958, -0.6104,  0.1775, -0.9954,\n",
       "          0.9937, -0.9966, -0.9731,  0.8012,  0.9443,  0.9992,  0.1727,  0.9924,\n",
       "          0.9989,  0.0722, -0.4268,  0.9878, -0.4814,  0.9241, -0.9997,  0.3852,\n",
       "          0.9989,  0.9334, -0.9893, -0.0772, -0.9683, -0.9848, -0.8839,  0.9798,\n",
       "         -0.2358,  0.8457,  0.9853, -0.9888, -0.9997,  0.9481, -0.9798, -0.9596,\n",
       "         -0.9765,  0.9985, -0.6875, -0.8859, -0.7813,  0.2480, -0.9881, -0.9939,\n",
       "          0.4989,  0.9922,  0.4589, -0.9952,  0.9986,  0.6848, -0.9992, -0.9503,\n",
       "         -0.9949, -0.0882, -0.8693,  0.9988,  0.3439,  0.8244,  0.3209, -0.9912,\n",
       "          0.9645, -0.9800, -0.9723, -0.2211,  0.9568,  0.9979,  0.9952, -0.7691,\n",
       "          0.9843,  0.9973, -0.1957,  0.9743, -0.9757,  0.9905,  0.8288, -0.9893,\n",
       "         -0.3133, -0.7588,  0.9995,  0.9959,  0.2445,  0.0029,  0.9868, -0.9399,\n",
       "          0.9977, -0.9995,  0.9847, -0.9991, -0.4474,  0.9198, -0.2715,  0.9996,\n",
       "         -0.2069,  0.9988, -0.9830, -0.9957,  0.5278,  0.4556,  0.9946, -0.9955,\n",
       "          0.8870, -0.5093,  0.2270,  0.8841, -0.9970,  0.9960, -0.8768,  0.9978,\n",
       "          0.9649, -0.4506, -0.9397, -0.9425,  0.3034, -0.9825, -0.7255,  0.8738,\n",
       "         -0.9457,  0.9992, -0.1272, -0.5480,  0.7377, -0.8748, -0.9756,  0.9500,\n",
       "         -0.7013,  0.9534,  0.8447,  0.4529,  0.9647,  0.2117, -0.9210,  0.9722,\n",
       "          0.8526,  0.2041,  0.9883,  0.7907, -0.7771, -0.9448, -0.9994, -0.6432,\n",
       "          0.9981, -0.8937, -0.9335,  0.3570, -0.9962,  0.7606, -0.7446, -0.6441,\n",
       "         -0.4508, -0.9986,  0.1090, -0.9688, -0.9603,  0.5410,  0.1773, -0.2186,\n",
       "         -0.9946,  0.5052,  0.9578,  0.3981,  0.8880, -0.5750, -0.9821,  0.5700,\n",
       "         -0.6027,  0.8283,  0.9890,  0.9970,  0.9615, -0.7697,  0.2187,  0.9898,\n",
       "          0.8368, -0.9992,  0.3838, -0.9681, -0.7511,  0.9970, -0.9880,  0.9705,\n",
       "          0.9991, -0.7666,  0.9992, -0.7885, -0.9924, -0.9894,  0.9981,  0.7465,\n",
       "          0.9990, -0.9235, -0.9466,  0.1959, -0.8231, -0.9982, -0.9980,  0.5410,\n",
       "          0.9762,  0.9982,  0.7282, -0.9830, -0.9440, -0.9906,  0.9992, -0.9348,\n",
       "          0.9785,  0.9819, -0.2930,  0.1810,  0.7722, -0.8802, -0.9710, -0.1409,\n",
       "         -0.9959, -0.9691, -0.9977,  0.9668, -0.9839, -0.9994,  0.9155,  0.9987,\n",
       "         -0.1169, -0.9971,  0.9626,  0.9904,  0.0060,  0.6063,  0.9409, -0.9996,\n",
       "          0.9992, -0.9930,  0.9534, -0.9795, -0.9956, -0.3670,  0.9801,  0.9958,\n",
       "         -0.9327,  0.6235, -0.9804, -0.0162,  0.3027,  0.9916, -0.9089, -0.0089,\n",
       "         -0.9282, -0.8054,  0.9373, -0.9626, -0.9668,  0.7896,  0.9979, -0.8967,\n",
       "          0.9995,  0.9975,  0.9999, -0.4248, -0.9509,  0.9972, -0.3500,  0.4444,\n",
       "         -0.5544,  0.3134,  0.9588, -0.0629, -0.2577, -0.9973,  0.9939, -0.6220,\n",
       "          0.6754,  0.7063, -0.9836,  0.0395,  0.9834, -0.9408,  0.9602, -0.9780,\n",
       "         -0.3813, -0.5117,  0.9980,  0.9230,  0.1199, -0.8725,  0.9972, -0.9967,\n",
       "          0.9720, -0.9984,  0.9969, -0.9247,  0.0072, -0.9357, -0.9910,  0.9954,\n",
       "          0.9883,  0.9530,  0.9875, -0.7725,  0.9862,  0.6576,  0.8262,  0.9799,\n",
       "          0.3689,  0.9976, -0.9884, -0.8071,  0.2361, -0.9963, -0.8822, -0.9990,\n",
       "         -0.0210, -0.9890, -0.8461, -0.1239, -0.8106,  0.4147, -0.6965, -0.8403,\n",
       "         -0.6660,  0.2250,  0.7164,  0.5190,  0.8291, -0.9861, -0.5520, -0.9990,\n",
       "         -0.9856, -0.1777,  0.9968, -0.9974,  0.9325, -0.9975, -0.1842, -0.3190,\n",
       "         -0.9297, -0.6248,  0.1783, -0.9927,  0.9540,  0.9172,  0.9992,  0.9878,\n",
       "          0.9792, -0.2895, -0.9449, -0.9889, -0.9964, -0.9995, -0.9956,  0.5822,\n",
       "          0.1512, -0.9955, -0.1582,  0.9783,  0.9976,  0.9648, -0.9936, -0.6633,\n",
       "         -0.9847, -0.2500,  0.9911, -0.4830, -0.9481,  0.8316, -0.0774,  0.9952,\n",
       "         -0.6176,  0.2722,  0.7464,  0.8023,  0.8308, -0.9931,  0.6574,  0.9991,\n",
       "          0.8968, -0.9946, -0.4442, -0.1172, -0.9937, -0.5411,  0.8894,  0.9936,\n",
       "         -0.9980, -0.1387, -0.8679,  0.9239,  0.8822,  0.9724,  0.9761,  0.7397,\n",
       "          0.9874, -0.3607,  0.4921,  0.9881,  0.5102, -0.9959,  0.9631, -0.2625,\n",
       "          0.3830, -0.9884,  0.9959, -0.8903,  0.9980,  0.7159, -0.2604, -0.9512,\n",
       "         -0.9897,  0.9859,  0.9986, -0.1642, -0.8316, -0.9878, -0.9977, -0.9783,\n",
       "         -0.9357,  0.6513, -0.9101, -0.9840,  0.5265,  0.3336,  0.9998,  0.9969,\n",
       "          0.9984, -0.8836, -0.8424,  0.9956, -0.4407,  0.7164, -0.1678, -0.9969,\n",
       "         -0.9830, -0.9962,  0.9565,  0.6815,  0.5010, -0.8619,  0.7230,  0.8110,\n",
       "         -0.9982, -0.8140, -0.9433,  0.9559,  0.9993, -0.9298,  0.8289, -0.9689,\n",
       "          0.0211,  0.8475,  0.9786,  0.9950, -0.9640,  0.3040, -0.4442, -0.4392,\n",
       "          0.9434,  0.9430, -0.9227, -0.3916,  0.9706, -0.9250,  0.6076, -0.5783,\n",
       "          0.8616,  0.9625,  0.9986,  0.7150,  0.9838, -0.2310,  0.8528,  0.9984,\n",
       "         -0.3255,  0.9338, -0.2872, -0.9439, -0.2417,  0.9726,  0.8687,  0.3492,\n",
       "         -0.1212, -0.9881,  0.9899,  0.9644,  0.9995, -0.4753,  0.9923, -0.5116,\n",
       "          0.8683,  0.7355,  0.7099,  0.6098,  0.6067,  0.9925,  0.9953, -0.9962,\n",
       "         -0.9408, -0.9992,  0.9994,  0.9485,  0.6851, -0.9975,  0.9934, -0.3590,\n",
       "          0.0682,  0.9887,  0.6213, -0.9933,  0.8246, -0.9559,  0.4600, -0.8315,\n",
       "          0.6590, -0.5237,  0.9922, -0.9724,  0.4217,  0.9984,  0.3151,  0.9722,\n",
       "          0.1885, -0.9935,  0.9696, -0.7475, -0.9863,  0.5729,  0.9944,  0.9652,\n",
       "          0.0444,  0.0990,  0.9539, -0.9747,  0.9950, -0.9970, -0.1903, -0.9638,\n",
       "          0.9955, -0.9749, -0.9979, -0.3906,  0.8157,  0.7413,  0.2413,  0.9978,\n",
       "         -0.7082, -0.9086, -0.8140, -0.5489, -0.9793, -0.9756, -0.0885, -0.9578,\n",
       "         -0.4972, -0.4224, -0.3640, -0.9396, -0.9818,  0.9977, -0.9510, -0.9565,\n",
       "          0.9988, -0.5196, -0.9979,  0.5923, -0.7890,  0.3798,  0.8848,  0.7645,\n",
       "          0.4011, -0.9999,  0.6092,  0.9971, -0.9926, -0.8853, -0.8861, -0.1712,\n",
       "         -0.6049,  0.5365,  0.9468, -0.5647,  0.7483, -0.4971,  0.9583, -0.2456,\n",
       "          0.4756, -0.6450, -0.4653, -0.9275, -0.9674, -0.9970, -0.9921,  0.9993,\n",
       "          0.9832,  0.9971, -0.8702, -0.4405,  0.7681,  0.9891, -0.9981, -0.8656,\n",
       "          0.4750,  0.6568,  0.2769, -0.9562, -0.0593, -0.9989, -0.7892,  0.4268,\n",
       "         -0.3884, -0.0126,  0.9989,  0.9143, -0.9866, -0.6273, -0.9907, -0.9893,\n",
       "          0.9975,  0.9510,  0.9800, -0.8805, -0.6749,  0.7988, -0.2018, -0.7835,\n",
       "         -0.9935, -0.9948, -0.9762,  0.7350, -0.9938, -0.9930,  0.9926,  0.9977,\n",
       "          0.8693, -0.9973, -0.7113,  0.9984,  0.9483,  0.9997,  0.3854,  0.9974,\n",
       "         -0.9523,  0.9798, -0.8351,  0.9991, -0.9939,  0.9988,  0.9996,  0.6145,\n",
       "          0.9873, -0.9892,  0.8121, -0.3139, -0.6369, -0.5115,  0.0240, -0.9476,\n",
       "         -0.8910,  0.9482, -0.9908,  0.9982, -0.1363,  0.2386,  0.9549, -0.2330,\n",
       "          0.8508,  0.9659, -0.9975,  0.5245,  0.9750,  0.9727,  0.9987,  0.9586,\n",
       "          0.9649, -0.8595, -0.9990,  0.2924, -0.8221, -0.5400, -0.9840,  0.9940,\n",
       "          0.9891, -0.9932,  0.5261,  0.1686,  0.7502,  0.9622,  0.9269, -0.4104,\n",
       "          0.8419,  0.8315,  0.9720, -0.9867,  0.8015, -0.9782,  0.3911,  0.9776,\n",
       "         -0.9674,  0.9984, -0.9936,  0.9901, -0.9087,  0.3463,  0.9821,  0.9750,\n",
       "         -0.8326,  0.9991,  0.8619, -0.6721, -0.7387, -0.9256, -0.8846, -0.0429]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "item = {key: torch.tensor(inputs[key]).view(1, -1) for key, val in inputs.items()}\n",
    "\n",
    "output = model(input_ids = item['input_ids'], attention_mask = item['attention_mask'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b14f5016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape, output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "368b7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in ['intent', 'device', 'mode', 'offset', 'endloc', 'landmark', 'singer', 'song']:\n",
    "    train_ja['槽值1'] = train_ja['槽值1'].str.replace(f'{tag}:', '')\n",
    "    train_ja['槽值2'] = train_ja['槽值2'].str.replace(f'{tag}:', '')\n",
    "    \n",
    "    train_cn['槽值1'] = train_cn['槽值1'].str.replace(f'{tag}:', '')\n",
    "    train_cn['槽值2'] = train_cn['槽值2'].str.replace(f'{tag}:', '')\n",
    "\n",
    "    train_en['槽值1'] = train_en['槽值1'].str.replace(f'{tag}:', '')\n",
    "    train_en['槽值2'] = train_en['槽值2'].str.replace(f'{tag}:', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "940c588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([\n",
    "    train_ja[['原始文本', '意图', '槽值1', '槽值2']],\n",
    "    train_cn[['原始文本', '意图', '槽值1', '槽值2']].sample(10000),\n",
    "    train_en[['原始文本', '意图', '槽值1', '槽值2']],\n",
    "],axis = 0)\n",
    "train_df = train_df.sample(frac=1.0)\n",
    "train_df['意图_encode'], lbl_ecode = pd.factorize(train_df['意图'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54d7532b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cdf9996b904863b8b7dfe1bdefd21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de85a0986aa4573817880617b71ff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165ccc3c37d34a93b9cb6b7b832355ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75ea58512a8410892a357d7beffdc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "config = AutoConfig.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f89aec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 数据集读取\n",
    "class XunFeiDataset(Dataset):\n",
    "    def __init__(self, encodings, intent):\n",
    "        self.encodings = encodings\n",
    "        self.intent = intent\n",
    "    \n",
    "    # 读取单个样本\n",
    "    def __getitem__(self, idx):        \n",
    "            \n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(int(self.intent[idx]))\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.intent)\n",
    "    \n",
    "class XunFeiModel(nn.Module):\n",
    "    def __init__(self, num_labels): \n",
    "        super(XunFeiModel,self).__init__() \n",
    "        self.model = model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "        logits = self.classifier(sequence_output[:,0,:].view(-1,768))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5f7dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoding = tokenizer(train_df['原始文本'].tolist()[:-500], truncation=True, padding=True, max_length=40)\n",
    "val_encoding = tokenizer(train_df['原始文本'].tolist()[-500:], truncation=True, padding=True, max_length=40)\n",
    "\n",
    "train_dataset = XunFeiDataset(train_encoding, train_df['意图_encode'].tolist()[:-500])\n",
    "val_dataset = XunFeiDataset(val_encoding, train_df['意图_encode'].tolist()[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61b9aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个读取到批量读取\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb0eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d3514a2695480e984d8dfb386cced6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = XunFeiModel(18)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c04c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "loss_fn = CrossEntropyLoss() # ingore index = -1\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    iter_num = 0\n",
    "    total_iter = len(train_loader)\n",
    "    for batch in train_loader:\n",
    "        # 正向传播\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        pred = model(\n",
    "            input_ids, \n",
    "            attention_mask\n",
    "        )\n",
    "        \n",
    "        loss = loss_fn(pred, label)\n",
    "        \n",
    "        # 反向梯度信息\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # 参数更新\n",
    "        optim.step()\n",
    "\n",
    "        iter_num += 1\n",
    "        \n",
    "        if(iter_num % 100 == 0):\n",
    "            print(\"iter_num: %d, loss: %.4f, %.2f%% %.4f\" % (\n",
    "                iter_num, loss.item(), iter_num/total_iter*100, \n",
    "                (pred.argmax(1) == label).float().data.cpu().numpy().mean(),\n",
    "            ))\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    label_acc = 0\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            pred = model(\n",
    "                input_ids, \n",
    "                attention_mask\n",
    "            )\n",
    "    \n",
    "            label_acc += (pred.argmax(1) == label).float().sum().item()\n",
    "    \n",
    "    label_acc = label_acc / len(val_dataloader.dataset)\n",
    "\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"Accuracy: %.4f\" % (label_acc))\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a56342",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    train()\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190db8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    model.eval()\n",
    "    test_label = []\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            test_label += list(pred.argmax(1).data.cpu().numpy())\n",
    "    return test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2aad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoding = tokenizer(test_en['原始文本'].tolist(), truncation=True, padding=True, max_length=40)\n",
    "test_dataset = XunFeiDataset(test_encoding, [0] * len(test_en))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_en_intent = prediction()\n",
    "\n",
    "test_encoding = tokenizer(test_ja['原始文本'].tolist(), truncation=True, padding=True, max_length=40)\n",
    "test_dataset = XunFeiDataset(test_encoding, [0] * len(test_ja))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_ja_intent = prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c141dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ja['意图'] = [lbl_ecode[x] for x in test_ja_intent]\n",
    "test_en['意图'] = [lbl_ecode[x] for x in test_en_intent]\n",
    "test_en['槽值1'] = np.nan\n",
    "test_en['槽值2'] = np.nan\n",
    "\n",
    "test_ja['槽值1'] = np.nan\n",
    "test_ja['槽值2'] = np.nan\n",
    "\n",
    "writer = pd.ExcelWriter('submit.xlsx')\n",
    "test_en[['意图', '槽值1', '槽值2']].to_excel(writer, sheet_name='英文_testA', index=None)\n",
    "test_ja[['意图', '槽值1', '槽值2']].to_excel(writer, sheet_name='日语_testA', index=None)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c1271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集读取\n",
    "class XunFeiDataset2(Dataset):\n",
    "    def __init__(self, encodings, label_i, label_j):\n",
    "        self.encodings = encodings\n",
    "        self.intent = intent\n",
    "        self.tag1 = tag1\n",
    "        self.tag2 = tag2\n",
    "    \n",
    "    # 读取单个样本\n",
    "    def __getitem__(self, idx):        \n",
    "        \n",
    "        tags_single = [0] + list(tags_single) + [0]\n",
    "        tags_single = tags_single + [0] * (32 - len(tags_single))\n",
    "        tags_single = tags_single[:maxlen]\n",
    "            \n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label_i'] = torch.tensor(int(self.label_i[idx]))\n",
    "        item['label_j'] = torch.tensor(int(self.label_j[idx]))\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label_i)\n",
    "    \n",
    "class XunFeiModel2(nn.Module):\n",
    "    def __init__(self, num_labels_i, num_labels_j): \n",
    "        super(XunFeiModel,self).__init__() \n",
    "\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier_i = nn.Linear(768, num_labels_i)\n",
    "        self.classifier_j = nn.Linear(768, num_labels_j)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "        \n",
    "        # 问诊方向\n",
    "        logits_i = self.classifier_i(sequence_output[:,0,:].view(-1,768))\n",
    "        # 疾病标签\n",
    "        logits_j = self.classifier_j(sequence_output[:,0,:].view(-1,768))\n",
    "        \n",
    "        return logits_i, logits_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1467ff55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
